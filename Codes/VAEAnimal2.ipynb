{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "\n",
    "#import F\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to cuda \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),transforms.ToTensor(),transforms.Normalize([0.5], [0.5]),transforms.Lambda(lambda x : x.to(device))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFolder = 'train'\n",
    "trainDataset = datasets.ImageFolder(trainFolder, transform=image_transform)\n",
    "\n",
    "# Select 1000 images only to train\n",
    "#trainDataset = torch.utils.data.Subset(trainDataset, np.random.choice(len(trainDataset), 1000, replace=False))\n",
    "\n",
    "validFolder = 'val'\n",
    "validDataset = datasets.ImageFolder(validFolder, transform=image_transform)\n",
    "\n",
    "# Select 100 images only to validate\n",
    "#validDataset = torch.utils.data.Subset(validDataset, np.random.choice(len(validDataset), 100, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:', len(trainDataset))\n",
    "print('Valid dataset:', len(validDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "batchSize = 16\n",
    "trainLoader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
    "validLoader = DataLoader(validDataset, batch_size=batchSize, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some images from the trainLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "for images, labels in trainLoader:\n",
    "    fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
    "    for i in range(4):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i].cpu().numpy().transpose(1,2,0).squeeze())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder accpting 128,128 images across 3 channels\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=6,kernel_size=5,stride=1,padding=0), # 128 -> 124\n",
    "            nn.Conv2d(in_channels=6,out_channels=9,kernel_size=5,stride=1,padding=0), # 124 -> 120\n",
    "            nn.BatchNorm2d(9),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=9,out_channels=12,kernel_size=10,stride=1,padding=0), # 120 -> 111\n",
    "            nn.Conv2d(in_channels=12,out_channels=15,kernel_size=10,stride=1,padding=0), # 111 -> 102\n",
    "            nn.BatchNorm2d(15),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=15,out_channels=18,kernel_size=15,stride=1,padding=0), # 102 -> 88\n",
    "            nn.Conv2d(in_channels=18,out_channels=21,kernel_size=15,stride=1,padding=0), # 88 -> 74\n",
    "            nn.BatchNorm2d(21),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=21,out_channels=24,kernel_size=20,stride=1,padding=0), # 74 -> 55\n",
    "            nn.Conv2d(in_channels=24,out_channels=27,kernel_size=20,stride=1,padding=0), # 55 -> 36\n",
    "            nn.BatchNorm2d(27),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=27,out_channels=30,kernel_size=25,stride=1,padding=0), # 36 -> 12\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(30*12*12, 100),\n",
    "        )\n",
    "        \n",
    "        # mean and log variance\n",
    "        self.mean = nn.Linear(100, 128)\n",
    "        self.logvar = nn.Linear(100, 128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 30*12*12),\n",
    "            nn.Unflatten(1, (30, 12, 12)),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=30,out_channels=27,kernel_size=25,stride=1,padding=0), # 12 -> 36\n",
    "            nn.ConvTranspose2d(in_channels=27,out_channels=24,kernel_size=20,stride=1,padding=0), # 36 -> 55\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=24,out_channels=21,kernel_size=20,stride=1,padding=0), # 55 -> 74\n",
    "            nn.ConvTranspose2d(in_channels=21,out_channels=18,kernel_size=15,stride=1,padding=0), # 74 -> 88\n",
    "            nn.BatchNorm2d(18),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=18,out_channels=15,kernel_size=15,stride=1,padding=0), # 88 -> 102\n",
    "            nn.ConvTranspose2d(in_channels=15,out_channels=12,kernel_size=10,stride=1,padding=0), # 102 -> 111\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=12,out_channels=9,kernel_size=10,stride=1,padding=0), # 111 -> 120\n",
    "            nn.ConvTranspose2d(in_channels=9,out_channels=6,kernel_size=5,stride=1,padding=0), # 120 -> 124\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=6,out_channels=3,kernel_size=5,stride=1,padding=0), # 124 -> 128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean = self.mean(x)\n",
    "        logvar = self.logvar(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x = self.decoder(z)\n",
    "        return x, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model = VariationalAutoEncoder().to(device)\n",
    "\n",
    "summary(model, (3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    MAE = F.mse_loss(recon_x, x.view(-1, 3, 128, 128), reduction='sum')\n",
    "    KLD = torch.mean(-0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()))\n",
    "    return MAE + 0.1*KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model \n",
    "\n",
    "model = VariationalAutoEncoder().to(device)\n",
    "\n",
    "# initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "n_epochs = 200\n",
    "log = Report(n_epochs)\n",
    "for epoch in range(n_epochs):\n",
    "    N = len(trainLoader)\n",
    "    for batchIndex, (imageSet, labelSet) in enumerate(trainLoader):\n",
    "        imageSet = imageSet.to(device)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.zero_grad()\n",
    "        recon_imageSet, mu, log_var = model(imageSet)\n",
    "        loss = loss_function(recon_imageSet, imageSet, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        log.record(epoch+(batchIndex+1)/N, train_loss=loss.item(), end='\\r')\n",
    "        \n",
    "    N = len(validLoader)\n",
    "    for batchIndex, (imageSet, labelSet) in enumerate(validLoader):\n",
    "        imageSet = imageSet.to(device)\n",
    "        \n",
    "        recon_imageSet, mu, log_var = model(imageSet)\n",
    "        loss = loss_function(recon_imageSet, imageSet, mu, log_var)\n",
    "        \n",
    "        log.record(epoch+(batchIndex+1)/N, valid_loss=loss.item(), end='\\r')\n",
    "    \n",
    "    log.report_avgs(epoch+1)\n",
    "    \n",
    "    #plot newly generated images64\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(32, 128).to(device)\n",
    "        samples = model.decoder(z).cpu()\n",
    "        grid = make_grid(samples, nrow=8, normalize=True)\n",
    "        show(grid, title=f'Epoch {epoch+1}')\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for images, labels in validLoader:\n",
    "            recon_images, _, _ = model(images.to(device))\n",
    "            break\n",
    "\n",
    "        N = 6\n",
    "        fig, axes = plt.subplots(2,N, figsize=(20,5))\n",
    "        for i in range(N):\n",
    "            axes[0,i].imshow(images[i].cpu().numpy().transpose(1,2,0).squeeze())\n",
    "            axes[1,i].imshow(recon_images[i].cpu().numpy().transpose(1,2,0).squeeze())\n",
    "            \n",
    "        plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.plot_epochs(['train_loss', 'valid_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See reconstructed images\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in validLoader:\n",
    "        recon_images, _, _ = model(images.to(device))\n",
    "        break\n",
    "\n",
    "    N = 6\n",
    "    fig, axes = plt.subplots(2,N, figsize=(20,5))\n",
    "    for i in range(N):\n",
    "        axes[0,i].imshow(images[i].cpu().numpy().transpose(1,2,0).squeeze())\n",
    "        axes[1,i].imshow(recon_images[i].cpu().numpy().transpose(1,2,0).squeeze())\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot newly generated images\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(32, 50).to(device)\n",
    "    samples = model.decoder(z).cpu()\n",
    "    grid = make_grid(samples, nrow=8, normalize=True)\n",
    "    show(grid, title=f'Epoch {epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
